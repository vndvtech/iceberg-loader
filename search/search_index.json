{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"iceberg-loader","text":"<p>A convenience wrapper around PyIceberg that simplifies data loading into Apache Iceberg tables. PyArrow-first, handles messy JSON, schema evolution, idempotent replace, upsert, batching, and streaming out of the box.</p> <p>Status: Actively developed and under testing. PRs are welcome! Currently tested against Hive Metastore; REST Catalog support is planned.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Arrow-first: <code>pa.Table</code>, <code>RecordBatch</code>, IPC.</li> <li>Messy JSON friendly: dict/list/mixed \u2192 JSON strings.</li> <li>Schema evolution (opt-in).</li> <li>Idempotent replace (<code>replace_filter</code>) and upsert.</li> <li>Commit interval for long streams.</li> <li>Maintenance helpers (expire snapshots).</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pip install \"iceberg-loader[all]\"\n</code></pre> <p>Or with uv:</p> <pre><code>uv pip install \"iceberg-loader[all]\"\n</code></pre>"},{"location":"#extras","title":"Extras","text":"Extra Description <code>hive</code> Hive Metastore support <code>s3fs</code> S3 filesystem support <code>pyiceberg-core</code> PyIceberg core <code>all</code> All extras"},{"location":"#compatibility","title":"Compatibility","text":"<ul> <li>Python: 3.10, 3.11, 3.12, 3.13, 3.14</li> <li>PyArrow: &gt;= 18.0.0</li> <li>PyIceberg: &gt;= 0.7.1</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code>import pyarrow as pa\nfrom pyiceberg.catalog import load_catalog\nfrom iceberg_loader import LoaderConfig, load_data_to_iceberg\n\ncatalog = load_catalog(\"default\")\ndata = pa.Table.from_pydict({\"id\": [1, 2], \"signup_date\": [\"2023-01-01\", \"2023-01-02\"]})\n\nconfig = LoaderConfig(write_mode=\"append\", partition_col=\"day(signup_date)\", schema_evolution=True)\nload_data_to_iceberg(data, (\"db\", \"users\"), catalog, config=config)\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#basic-example","title":"Basic Example","text":"<pre><code>import pyarrow as pa\nfrom pyiceberg.catalog import load_catalog\nfrom iceberg_loader import LoaderConfig, load_data_to_iceberg\n\ncatalog = load_catalog(\"default\")\n\ndata = pa.Table.from_pydict({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"created_at\": [1672531200000, 1672617600000, 1672704000000],\n    \"signup_date\": [\"2023-01-01\", \"2023-01-01\", \"2023-01-02\"]\n})\n\nconfig = LoaderConfig(write_mode=\"append\", partition_col=\"day(signup_date)\", schema_evolution=True)\nresult = load_data_to_iceberg(\n    table_data=data,\n    table_identifier=(\"my_db\", \"my_table\"),\n    catalog=catalog,\n    config=config,\n)\n\nprint(result)\n# {'rows_loaded': 3, 'write_mode': 'append', 'partition_col': 'day(signup_date)', ...}\n</code></pre>"},{"location":"#idempotent-load-replace-partition","title":"Idempotent Load (Replace Partition)","text":"<p>Safely re-load data for a specific day (avoiding duplicates):</p> <pre><code>config = LoaderConfig(\n    write_mode=\"append\",\n    replace_filter=\"signup_date == '2023-01-01'\",\n    partition_col=\"day(signup_date)\",\n)\n\nload_data_to_iceberg(table_data=data, table_identifier=(\"my_db\", \"my_table\"), catalog=catalog, config=config)\n</code></pre>"},{"location":"#upsert-merge-into","title":"Upsert (Merge Into)","text":"<p>Merge operation (update existing rows, insert new ones) based on key columns. Requires PyIceberg &gt;= 0.7.0.</p> <pre><code>config = LoaderConfig(write_mode=\"upsert\", join_cols=[\"id\"])\nload_data_to_iceberg(table_data=data, table_identifier=(\"my_db\", \"my_table\"), catalog=catalog, config=config)\n</code></pre>"},{"location":"#batch-loading","title":"Batch Loading","text":"<p>For large datasets, use <code>load_batches_to_iceberg</code> with an iterator of RecordBatches:</p> <pre><code>from iceberg_loader import load_batches_to_iceberg\n\ndef batch_generator():\n    for i in range(10):\n        yield some_record_batch\n\nresult = load_batches_to_iceberg(\n    batch_iterator=batch_generator(),\n    table_identifier=(\"my_db\", \"large_table\"),\n    catalog=catalog,\n    config=LoaderConfig(write_mode=\"append\", commit_interval=100),\n)\n</code></pre>"},{"location":"#stream-loading-arrow-ipc","title":"Stream Loading (Arrow IPC)","text":"<p>Load data directly from an Apache Arrow IPC stream:</p> <pre><code>from iceberg_loader import load_ipc_stream_to_iceberg\n\nresult = load_ipc_stream_to_iceberg(\n    stream_source=\"data.arrow\",\n    table_identifier=(\"my_db\", \"stream_table\"),\n    catalog=catalog,\n    config=LoaderConfig(write_mode=\"append\"),\n)\n</code></pre>"},{"location":"#custom-settings","title":"Custom Settings","text":"<p>Override default table properties:</p> <pre><code>custom_props = {\n    'write.parquet.compression-codec': 'snappy',\n    'history.expire.min-snapshots-to-keep': 5,\n}\n\nconfig = LoaderConfig(table_properties=custom_props, write_mode=\"append\")\nload_data_to_iceberg(..., config=config)\n</code></pre>"},{"location":"#override-default-table-properties-globally","title":"Override Default Table Properties Globally","text":"<p>Built-in defaults live in <code>iceberg_loader.core.config.TABLE_PROPERTIES</code> (format version, Parquet compression, commit retries). To tweak them, copy the dictionary, override keys, and pass into <code>LoaderConfig</code>:</p> <pre><code>from pyiceberg.catalog import load_catalog\nfrom iceberg_loader import LoaderConfig, load_data_to_iceberg\nfrom iceberg_loader.core.config import TABLE_PROPERTIES\n\ncatalog = load_catalog(\"default\")\n\ncustom_properties = {**TABLE_PROPERTIES, \"write.parquet.compression-codec\": \"gzip\"}\n\nconfig = LoaderConfig(\n    write_mode=\"append\",\n    table_properties=custom_properties,\n)\n\nload_data_to_iceberg(table_data, (\"default\", \"events\"), catalog, config=config)\n</code></pre>"},{"location":"#maintenance-helper","title":"Maintenance Helper","text":"<pre><code>from iceberg_loader import expire_snapshots\n\ntable = catalog.load_table((\"db\", \"users\"))\nexpire_snapshots(table, keep_last=2)\n</code></pre>"},{"location":"#adding-load-timestamp","title":"Adding Load Timestamp","text":"<p>You can automatically add a timestamp column (e.g. <code>_load_dttm</code>) to every row to track when it was loaded. This is useful for ETL audit trails or partitioning by load time.</p> <pre><code>from datetime import datetime\n\n# Will add column '_load_dttm' with current time and partition by hour\nconfig = LoaderConfig(\n    write_mode=\"append\",\n    load_timestamp=datetime.now(),\n    partition_col=\"hour(_load_dttm)\",\n)\n\n# You can also customize the column name and day-transform it\nconfig_custom = LoaderConfig(\n    write_mode=\"append\",\n    load_timestamp=datetime(2025, 1, 1),\n    load_ts_col=\"etl_ts\",\n    partition_col=\"day(etl_ts)\",\n)\n</code></pre>"},{"location":"#loaderconfig-reference","title":"LoaderConfig Reference","text":"Parameter Type Default Description <code>write_mode</code> <code>'append'</code> | <code>'overwrite'</code> | <code>'upsert'</code> <code>'overwrite'</code> Data write mode <code>partition_col</code> <code>str \\| None</code> <code>None</code> Partition column or transform (e.g., <code>month(ts)</code>, <code>bucket(16,id)</code>); prefer day/hour on timestamps <code>replace_filter</code> <code>str \\| None</code> <code>None</code> SQL-style filter for idempotent loads <code>schema_evolution</code> <code>bool</code> <code>False</code> Auto-add new columns <code>commit_interval</code> <code>int</code> <code>0</code> Commit every N batches (0 = single transaction) <code>join_cols</code> <code>list[str] \\| None</code> <code>None</code> Merge keys for upsert <code>table_properties</code> <code>dict \\| None</code> <code>None</code> Custom Iceberg table properties <code>load_timestamp</code> <code>datetime \\| None</code> <code>None</code> If set, adds <code>_load_dttm</code> column with this value <code>load_ts_col</code> <code>str</code> <code>'_load_dttm'</code> Name of the load timestamp column"},{"location":"#api-reference","title":"API Reference","text":""},{"location":"#load_batches_to_iceberg","title":"<code>load_batches_to_iceberg()</code>","text":"<p>Main function for loading a stream of batches into an Iceberg table.</p> <pre><code>def load_batches_to_iceberg(\n    batch_iterator: Iterator[pa.RecordBatch] | pa.RecordBatchReader,\n    table_identifier: tuple[str, str],\n    catalog: Catalog,\n    config: LoaderConfig | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>All load parameters (write_mode, partition_col, replace_filter, join_cols, schema_evolution, commit_interval, table_properties) are passed only via <code>LoaderConfig</code>. See the table in LoaderConfig Reference above.</p>"},{"location":"#return-value","title":"Return Value","text":"<p>Dictionary with loading results:</p> <pre><code>{\n    'rows_loaded': int,\n    'batches_processed': int,\n    'write_mode': str,\n    'partition_col': str,\n    'table_location': str,\n    'snapshot_id': int | str,\n    'new_table_created': bool,\n}\n</code></pre>"},{"location":"#usage-examples","title":"Usage Examples","text":"<p>Basic stream loading:</p> <pre><code>def generate_batches():\n    for i in range(100):\n        data = {\"id\": [i], \"value\": [f\"row_{i}\"]}\n        yield pa.RecordBatch.from_pydict(data)\n\nresult = load_batches_to_iceberg(\n    batch_iterator=generate_batches(),\n    table_identifier=(\"db\", \"table\"),\n    catalog=catalog,\n    config=LoaderConfig(write_mode=\"append\")\n)\n</code></pre> <p>With commits for memory management:</p> <pre><code>result = load_batches_to_iceberg(\n    batch_iterator=large_batch_stream,\n    table_identifier=(\"db\", \"large_table\"),\n    catalog=catalog,\n    config=LoaderConfig(\n        commit_interval=50,\n        schema_evolution=True\n    )\n)\n</code></pre> <p>Idempotent partition loading:</p> <pre><code>result = load_batches_to_iceberg(\n    batch_iterator=daily_batches,\n    table_identifier=(\"db\", \"events\"),\n    catalog=catalog,\n    config=LoaderConfig(\n        write_mode=\"append\",\n        replace_filter=\"event_date == '2023-12-09'\",\n        partition_col=\"day(event_date)\"\n    )\n)\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p>See the Examples page for runnable demos covering streaming, upsert, schema evolution, messy JSON, and more.</p>"},{"location":"#how-we-version","title":"How we version","text":"<ul> <li>Semantic Versioning from <code>0.1.x</code>: MINOR = new compatible features, PATCH = fixes, MAJOR = breaking changes.</li> <li>Public API: <code>LoaderConfig</code>, <code>load_data_to_iceberg</code>, <code>load_batches_to_iceberg</code>, <code>load_ipc_stream_to_iceberg</code>; other modules are internal.</li> <li>Prefer partition transforms for timestamps (<code>day(ts)</code>, <code>hour(ts)</code>, <code>day(_load_dttm)</code>) to avoid unbounded partition counts.</li> <li>LoaderConfig validates partition expressions and forbids unsafe mixes (e.g., <code>replace_filter</code> with <code>upsert</code>, identity partition on <code>_load_dttm</code>).</li> </ul>"},{"location":"#release-checklist","title":"Release checklist","text":"<ul> <li>Align versions in <code>pyproject.toml</code> and <code>src/iceberg_loader/__about__.py</code>.</li> <li>Update <code>RELEASE.md</code> with highlights/breaking changes.</li> <li>Run <code>uv lock --locked</code> and commit <code>uv.lock</code> if it changes.</li> <li>Run lint (<code>uv run ruff check .</code>), types (<code>uv run mypy src/iceberg_loader tests</code>), and tests (<code>uv run python -m pytest</code>).</li> <li>Tag and push (<code>git tag -a vX.Y.Z -m \"Release X.Y.Z\"</code>), let CI publish.</li> </ul>"},{"location":"#development","title":"Development","text":"<p>This project uses Hatch as build backend. For local workflows prefer uv.</p>"},{"location":"#run-tests","title":"Run Tests","text":"<pre><code>uv run python -m pytest\n</code></pre>"},{"location":"#linting","title":"Linting","text":"<pre><code>uv run ruff check\nuv run ruff format --check\nuv run mypy\n</code></pre>"},{"location":"#release","title":"Release","text":"<pre><code>hatch build\ntwine check dist/*\ntwine upload --repository testpypi dist/*\ntwine upload dist/*\n</code></pre>"},{"location":"#license","title":"License","text":"<p><code>iceberg-loader</code> is distributed under the terms of the MIT license.</p>"},{"location":"examples/","title":"Examples","text":"<p>Runnable examples demonstrating various features of <code>iceberg-loader</code>. All examples are located in the <code>examples/</code> directory.</p>"},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<p>You need a running Iceberg catalog (e.g., Hive Metastore) and MinIO/S3. Use the bundled <code>docker-compose.yml</code> to start a local stack (run from repo root):</p> <pre><code>cd examples\ndocker-compose up -d\n</code></pre> <p>Then run examples from the same <code>examples/</code> directory (see commands below). With <code>uv</code> you can prefix any command as <code>uv run python &lt;script.py&gt;</code>.</p>"},{"location":"examples/#core-examples","title":"Core Examples","text":"Example Description <code>load_with_commits.py</code> Commit interval for long streams <code>load_batches.py</code> Loading data in batches using <code>load_batches_to_iceberg</code> <code>load_upsert.py</code> Upsert (merge) by key columns <code>advanced_scenarios.py</code> Schema evolution, custom types, partitioning <code>load_complex_json.py</code> Messy JSON handling <code>compare_complex_json_fail.py</code> PyArrow fails on mixed types, iceberg-loader succeeds"},{"location":"examples/#other-examples","title":"Other Examples","text":"Example Description <code>load_stream.py</code> Arrow IPC stream loading <code>load_from_api.py</code> Simulated REST API ingestion <code>maintenance_example.py</code> Snapshot expiration"},{"location":"examples/#running","title":"Running","text":"<p>Run from the <code>examples/</code> directory:</p> <pre><code>cd examples\n\n# Core\npython load_with_commits.py\npython load_batches.py\npython load_upsert.py\npython advanced_scenarios.py\npython load_complex_json.py\n\n# Other\npython load_stream.py\npython load_from_api.py\npython maintenance_example.py\n</code></pre> <p>With uv:</p> <pre><code>uv run python load_with_commits.py\nuv run python load_batches.py\nuv run python load_upsert.py\nuv run python advanced_scenarios.py\nuv run python load_complex_json.py\nuv run python load_stream.py\nuv run python load_from_api.py\nuv run python maintenance_example.py\n</code></pre>"},{"location":"examples/#example-highlights","title":"Example Highlights","text":""},{"location":"examples/#loading-data-in-batches","title":"Loading Data in Batches","text":"<p>Load data from an iterator of RecordBatches:</p> <pre><code>import pyarrow as pa\nfrom iceberg_loader import LoaderConfig, load_batches_to_iceberg\n\ndef generate_batches():\n    for i in range(10):\n        data = {\n            'id': list(range(i * 100, (i + 1) * 100)),\n            'name': [f'Item_{j}' for j in range(100)],\n        }\n        yield pa.RecordBatch.from_pydict(data)\n\nconfig = LoaderConfig(write_mode=\"append\", schema_evolution=True)\nresult = load_batches_to_iceberg(\n    batch_iterator=generate_batches(),\n    table_identifier=(\"db\", \"items\"),\n    catalog=catalog,\n    config=config,\n)\n</code></pre>"},{"location":"examples/#commit-interval-streaming","title":"Commit Interval (Streaming)","text":"<p>Use <code>commit_interval</code> to flush batches periodically during long-running streams:</p> <pre><code>from iceberg_loader import LoaderConfig, load_batches_to_iceberg\n\nconfig = LoaderConfig(write_mode=\"append\", commit_interval=100)\nresult = load_batches_to_iceberg(\n    batch_iterator=my_batch_generator(),\n    table_identifier=(\"db\", \"events\"),\n    catalog=catalog,\n    config=config,\n)\n</code></pre>"},{"location":"examples/#upsert-merge","title":"Upsert (Merge)","text":"<p>Perform merge operations (update existing, insert new) based on key columns:</p> <pre><code>config = LoaderConfig(write_mode=\"upsert\", join_cols=[\"id\"])\nload_data_to_iceberg(data, (\"db\", \"users\"), catalog, config=config)\n</code></pre>"},{"location":"examples/#dynamic-configuration-multi-table-load","title":"Dynamic Configuration (Multi-table Load)","text":"<p>When loading multiple tables in a loop, you can dynamically switch <code>LoaderConfig</code> for each table:</p> <pre><code># Define configurations\nconfig_overwrite = LoaderConfig(write_mode='overwrite', schema_evolution=True)\nconfig_upsert = LoaderConfig(write_mode='upsert', join_cols=['id'], schema_evolution=True)\n\n# Map endpoints/tables to specific configs\nendpoint_configs = {\n    'customers': config_overwrite,\n    'orders': config_upsert,\n}\n\nfor endpoint in endpoints:\n    # Use specific config or default to append\n    current_config = endpoint_configs.get(endpoint, LoaderConfig(write_mode='append'))\n\n    load_data_to_iceberg(\n        table_data=data,\n        table_identifier=('default', endpoint),\n        catalog=catalog,\n        config=current_config\n    )\n</code></pre>"},{"location":"examples/#messy-json","title":"Messy JSON","text":"<p>iceberg-loader auto-serializes mixed/nested types to JSON strings when PyArrow would fail:</p> <pre><code>from iceberg_loader import LoaderConfig, load_data_to_iceberg\nfrom iceberg_loader.utils.arrow import create_arrow_table_from_data\n\ndata = [\n    {\"id\": 1, \"complex_field\": {\"a\": 1, \"b\": \"nested\"}},\n    {\"id\": 2, \"complex_field\": {\"a\": 2, \"b\": \"another\", \"c\": [1, 2]}},\n    {\"id\": 3, \"complex_field\": [1, 2, 3]},\n]\n\narrow_table = create_arrow_table_from_data(data)\n\nconfig = LoaderConfig(write_mode=\"append\")\nload_data_to_iceberg(data, (\"db\", \"events\"), catalog, config=config)\n</code></pre>"},{"location":"examples/#schema-evolution","title":"Schema Evolution","text":"<p>Automatically add new columns when data schema changes:</p> <pre><code>config = LoaderConfig(write_mode=\"append\", schema_evolution=True)\nload_data_to_iceberg(data_with_new_columns, (\"db\", \"table\"), catalog, config=config)\n</code></pre>"},{"location":"examples/#partitioning","title":"Partitioning","text":"<p>Create partitioned tables with transform expressions:</p> <pre><code>from datetime import datetime\n\nconfig = LoaderConfig(\n    write_mode=\"append\",\n    partition_col=\"month(event_date)\",  # or day(), year(), bucket(16, id), etc.\n)\nload_data_to_iceberg(data, (\"db\", \"events\"), catalog, config=config)\n\ningestion_config = LoaderConfig(\n    write_mode=\"append\",\n    load_timestamp=datetime.now(),\n    partition_col=\"hour(_load_dttm)\",\n)\n</code></pre>"}]}